{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac891d1b-65af-48f5-9c72-9e4a06346f1a",
   "metadata": {},
   "source": [
    "# Mixfeat session-level experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae84140-b1d2-4b35-8931-42ed84a4d159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# update the path so we can directly import code from the DVlog\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"DVlog\"))))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"DVlog\")))\n",
    "\n",
    "from DVlog.utils.bias_mitigations import apply_oversampling, apply_mixfeat_oversampling\n",
    "from DVlog.utils.metrics import calculate_performance_measures, calculate_gender_performance_measures, calculate_fairness_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e3e16-b07e-487a-bac3-c53ab6e8e638",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37868a-0844-4ab3-a602-952538cdc246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations_file = Path(r\"../DVlog/dataset/dvlog_labels_v2.csv\")\n",
    "embeddings_path = Path(\"../DVlog/dataset/sent-embeddings-dataset\")\n",
    "feature_name = \"sent_mpnet_keyw\"\n",
    "\n",
    "seed = 42\n",
    "random_seeds = [0, 1, 42, 1123, 3107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee4490-f4a2-4a48-a727-46a565976d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in the annotation labels\n",
    "df_annotations = pd.read_csv(annotations_file)\n",
    "df_annotations.reset_index(drop=True, inplace=True)\n",
    "df_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2269ed5-cb8d-4166-9fe6-a009ded76988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop over each row and compute the average embeddings\n",
    "df_annotations[\"avg_embed\"] = None\n",
    "\n",
    "# loop over each row and retrieve the embeddings\n",
    "seq_length = 104\n",
    "\n",
    "for idx, row in df_annotations.iterrows():\n",
    "    # get the texts\n",
    "    video_id = row.video_id\n",
    "    \n",
    "    # setup the path to the file\n",
    "    embedding_path = os.path.join(embeddings_path, str(video_id), f\"{feature_name}.npy\")\n",
    "    embedding = np.load(embedding_path).astype(np.float32)\n",
    "\n",
    "    # apply the padding\n",
    "    padded_embedding = embedding[:seq_length]\n",
    "\n",
    "    # get the average over the whole embedding\n",
    "    avg_embedding = np.mean(padded_embedding, axis=0)\n",
    "    # std_embedding = np.std(padded_embedding, axis=0)\n",
    "\n",
    "    # put the embedding back\n",
    "    df_annotations.at[idx, \"avg_embed\"] = avg_embedding\n",
    "\n",
    "df_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9cfab-45ed-47ab-88d0-907f22788260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the train and validation datasets\n",
    "train_indices = df_annotations[df_annotations[\"dataset\"] == \"train\"].index\n",
    "val_indices = df_annotations[df_annotations[\"dataset\"] == \"val\"].index\n",
    "\n",
    "# prepare the features and labels\n",
    "avg_features = np.stack(df_annotations[\"avg_embed\"].values)\n",
    "labels = df_annotations[\"label\"].values\n",
    "genders = df_annotations[\"gender\"].values\n",
    "\n",
    "# create the train and validation sets\n",
    "X_train, X_val = avg_features[train_indices], avg_features[val_indices]\n",
    "y_train, y_val = labels[train_indices], labels[val_indices]\n",
    "\n",
    "# combine the train and validation sets\n",
    "X = np.vstack((X_train, X_val))\n",
    "y = np.hstack((y_train, y_val))\n",
    "\n",
    "# Create a test_fold array: -1 for training set, 0 for validation set\n",
    "test_fold = np.concatenate([\n",
    "    -1 * np.ones(len(X_train), dtype=int),\n",
    "    np.zeros(len(X_val), dtype=int)\n",
    "])\n",
    "\n",
    "print(X.shape, y.shape, test_fold.shape)\n",
    "\n",
    "# Create PredefinedSplit object\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed143f12-df7f-4606-9ad9-60bc683e8236",
   "metadata": {},
   "source": [
    "## setup the gridsearch with the parameters\n",
    "- C (Regularization Parameter): Controls the trade-off between achieving a low error on the training data and minimizing the norm of the weights. A small value for C makes the decision surface smooth, while a large value of C aims to classify all training examples correctly.\n",
    "\n",
    "- Gamma (Kernel Coefficient): Defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. It is applicable for 'rbf', 'poly', and 'sigmoid' kernels.\n",
    "\n",
    "- Kernel: Specifies the kernel type to be used in the algorithm. Common kernels are 'linear', 'poly' (polynomial), 'rbf' (radial basis function), and 'sigmoid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf8be3-bcda-43a5-9eb6-52ce7778cf13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the SVM and parameter grid\n",
    "svm = SVC(random_state=seed)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Set up and run GridSearchCV\n",
    "grid_search_avg = GridSearchCV(estimator=svm, param_grid=param_grid, cv=ps, verbose=2, n_jobs=-1)\n",
    "grid_search_avg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512909fa-6767-4d73-aa21-ce94624412ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output best parameters and score\n",
    "best_params = grid_search_avg.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best validation score: \", grid_search_avg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57df055-aa1e-4fab-b90f-786c62677410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the function for automatically retrieve all metrics\n",
    "def evaluate_model(y_true, y_pred, protected):\n",
    "\n",
    "    # calculate the performance metrics\n",
    "    w_precision, w_recall, w_fscore, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    # calculate the fairness metrics\n",
    "    eq_oppor, eq_acc, pred_equal, _, _ = calculate_fairness_measures(y_true, y_pred, protected, unprivileged='m')\n",
    "    \n",
    "    # eq_oppor, eq_acc, fairl_eq_odds, unpriv_stats, priv_stats = calculate_fairness_measures(y_true, y_pred, protected, 'm')\n",
    "    gender_metrics = calculate_gender_performance_measures(y_true, y_pred, protected)\n",
    "\n",
    "    measure_dict = {\n",
    "        \"precision\": w_precision,\n",
    "        \"recall\": w_recall,\n",
    "        \"fscore\": w_fscore,\n",
    "        f\"{gender_metrics[0][0]}_fscore\": gender_metrics[0][3],\n",
    "        f\"{gender_metrics[1][0]}_fscore\": gender_metrics[1][3],\n",
    "        \"eq_oppor\": eq_oppor,\n",
    "        \"eq_acc\": eq_acc,\n",
    "        \"pred_eq\": pred_equal}\n",
    "    return measure_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bbd62-73fc-4929-9b76-ba46ea40c0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaulate this model on the test set\n",
    "test_indices = df_annotations[df_annotations[\"dataset\"] == \"test\"].index\n",
    "X_test, y_test, protec_test = avg_features[test_indices], labels[test_indices], genders[test_indices]\n",
    "\n",
    "# Evaluate the best model (avg)\n",
    "best_svm = grid_search_avg.best_estimator_\n",
    "y_pred = best_svm.predict(X_test)\n",
    "base_eval_dict_avg = evaluate_model(y_test, y_pred, protec_test)\n",
    "\n",
    "# Evaluate the best model (std)\n",
    "# best_svm = grid_search_std.best_estimator_\n",
    "# y_pred = best_svm.predict(X_test_std)\n",
    "# base_eval_dict_std = evaluate_model(y_test, y_pred, protec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f8661-c5f1-486e-b0e4-5e01e17a0c39",
   "metadata": {},
   "source": [
    "## Setup the bias mitigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333cecd-cbc3-4886-8bd2-51cc30dcce2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixfeat_options = ['oversample', 'group_upsample', 'mixgender_upsample', 'subgroup_upsample', 'synthetic', 'synthetic_mixgendered']\n",
    "results = [(\"base_model_avg\", base_eval_dict_avg)]\n",
    "\n",
    "# get the training section\n",
    "df_train = df_annotations[df_annotations[\"dataset\"] == \"train\"]\n",
    "\n",
    "# take the training_df and do the oversampling for each option\n",
    "for seed in random_seeds:\n",
    "    for option in mixfeat_options:\n",
    "        print(f\"Processing: {option} with seed: {seed}\")\n",
    "\n",
    "        # get the training section\n",
    "        df_copy = df_train.copy()\n",
    "        if option == 'oversample':\n",
    "            training_df = apply_oversampling(df_copy, seed)\n",
    "            X = np.stack(training_df[\"avg_embed\"].values)\n",
    "\n",
    "        else:\n",
    "            training_df = apply_mixfeat_oversampling(df_copy, option, 1, seed)\n",
    "\n",
    "            # extract the training data and apply the mixfeat operation whenever possible\n",
    "            X = []\n",
    "            for _, row in training_df.iterrows():\n",
    "                if row.mixfeat:\n",
    "                    idx1, idx2 = row.mixfeat\n",
    "                    prob = row.mixfeat_probs[0]\n",
    "\n",
    "                    # get the embeddings from the dataframe\n",
    "                    embedding1 = df_train.loc[df_train['video_id'] == idx1][\"avg_embed\"].values[0]\n",
    "                    embedding2 = df_train.loc[df_train['video_id'] == idx2][\"avg_embed\"].values[0]\n",
    "\n",
    "                    final_embedding = (embedding1 * prob) + (embedding2 * (1 - prob))\n",
    "                    X.append(final_embedding)\n",
    "                else:\n",
    "                    X.append(row.avg_embed)\n",
    "\n",
    "            # get all the information and train the model\n",
    "            X = np.array(X)\n",
    "\n",
    "        # retrieve the label information\n",
    "        y = training_df[\"label\"].values\n",
    "\n",
    "        # train an SVM model\n",
    "        svm = SVC(**best_params, random_state=seed)\n",
    "        svm.fit(X, y)\n",
    "\n",
    "        # evaluate the model\n",
    "        y_pred = svm.predict(X_test)\n",
    "        eval_dict = evaluate_model(y_test, y_pred, protec_test)\n",
    "        results.append((option, eval_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a8e09b-8f04-4d5d-889f-3320ecf55e80",
   "metadata": {},
   "source": [
    "## Setup the post-processing bias mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca89f58-5566-4c03-bbcd-3b35f181e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results from the normal textual model\n",
    "best_svm = grid_search_avg.best_estimator_\n",
    "y_pred_train = best_svm.predict(X_train)\n",
    "y_pred_test = best_svm.predict(X_test)\n",
    "\n",
    "# set \n",
    "protec_train = genders[train_indices]\n",
    "protected_train = np.where(protec_train == \"m\", 0, 1)\n",
    "protected_test = np.where(protec_test == \"m\", 0, 1)\n",
    "    \n",
    "# Function to apply EqOddsPostprocessing\n",
    "def apply_eqodds(y_train_true, y_train_pred, y_test_pred, protected_attr_train, protected_attr_test, seed):\n",
    "    # Create BinaryLabelDataset for training data\n",
    "    dataset_train_true = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=pd.DataFrame({\n",
    "        'label': y_train_true,\n",
    "        'protected': protected_attr_train\n",
    "    }), label_names=['label'], protected_attribute_names=['protected'])\n",
    "\n",
    "    dataset_train_pred = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=pd.DataFrame({\n",
    "        'label': y_train_pred,\n",
    "        'protected': protected_attr_train\n",
    "    }), label_names=['label'], protected_attribute_names=['protected'])\n",
    "\n",
    "    # Create BinaryLabelDataset for test data\n",
    "    dataset_test_pred = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=pd.DataFrame({\n",
    "        'label': y_test_pred,\n",
    "        'protected': protected_attr_test\n",
    "    }), label_names=['label'], protected_attribute_names=['protected'])\n",
    "\n",
    "    # Apply EqOddsPostprocessing\n",
    "    eq_odds = EqOddsPostprocessing(unprivileged_groups=[{'protected': 0}],\n",
    "                                   privileged_groups=[{'protected': 1}], seed=seed)\n",
    "\n",
    "    eq_odds = eq_odds.fit(dataset_train_true, dataset_train_pred)\n",
    "    dataset_transf_test_pred = eq_odds.predict(dataset_test_pred)\n",
    "\n",
    "    # Extract the adjusted predictions\n",
    "    adjusted_pred = dataset_transf_test_pred.labels.ravel()\n",
    "    return adjusted_pred\n",
    "\n",
    "# \n",
    "for seed in random_seeds:\n",
    "    new_preds = apply_eqodds(y_train, y_pred_train, y_pred_test, protected_train, protected_test, seed)\n",
    "    eval_dict = evaluate_model(y_test, new_preds, protec_test)\n",
    "    results.append((\"eqodds\", eval_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc61232-1f22-4b1f-9c01-904b9db0c723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract data into a structured format\n",
    "extracted_data = []\n",
    "for name, result in results:\n",
    "    data = {\n",
    "        \"name\": name,\n",
    "        \"Precision\": np.round(result[\"precision\"], 3),\n",
    "        \"Recall\": np.round(result[\"recall\"], 3),\n",
    "        \"F-score\": np.round(result[\"fscore\"], 3),\n",
    "        \"Male F-score\": np.round(result[\"m_fscore\"], 3),\n",
    "        \"Female F-score\": np.round(result[\"f_fscore\"], 3),\n",
    "        \"eq_oppor\": np.round(result[\"eq_oppor\"], 2),\n",
    "        \"eq_acc\": np.round(result[\"eq_acc\"], 2),\n",
    "        \"pred_eq\": np.round(result[\"pred_eq\"], 2)\n",
    "    }\n",
    "    extracted_data.append(data)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame and display it\n",
    "df = pd.DataFrame(extracted_data)\n",
    "df.groupby(\"name\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126d807-8da6-42d8-9dbb-04d8ca0624c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby(\"name\").std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
