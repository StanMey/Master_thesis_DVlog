{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091b38d0-d72d-40e9-a9b5-671b3fad3601",
   "metadata": {},
   "source": [
    "# Equalised Odds post-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b8e150-dc3d-47bc-87c2-4525afabac9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from confection import Config\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from itertools import chain\n",
    "\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# update the path so we can directly import code from the DVlog\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"DVlog\"))))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"DVlog\")))\n",
    "\n",
    "from DVlog.evaluate import evaluate_model\n",
    "from DVlog.models.model import UnimodalDVlogModel\n",
    "from DVlog.utils.dataloaders import MultimodalEmbeddingsDataset\n",
    "from DVlog.utils.metrics import calculate_performance_measures, calculate_gender_performance_measures, calculate_fairness_measures\n",
    "from DVlog.utils.util import ConfigDict, validate_config, process_config, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3868505-2902-40bc-80d5-fd3a0e004556",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf5343b-bec4-413b-aa5c-e38773888ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to apply EqOddsPostprocessing\n",
    "def apply_eqodds(y_train_true, y_train_pred, y_val_pred, y_test_pred, protected_attr_train, protected_attr_val, protected_attr_test, seed):\n",
    "    # Create BinaryLabelDataset for training data\n",
    "    dataset_train_true = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=pd.DataFrame({\n",
    "        'label': y_train_true,\n",
    "        'protected': protected_attr_train\n",
    "    }), label_names=['label'], protected_attribute_names=['protected'])\n",
    "\n",
    "    dataset_train_pred = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=pd.DataFrame({\n",
    "        'label': y_train_pred,\n",
    "        'protected': protected_attr_train\n",
    "    }), label_names=['label'], protected_attribute_names=['protected'])\n",
    "    \n",
    "    # Create BinaryLabelDataset for val data\n",
    "    dataset_val_pred = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=pd.DataFrame({\n",
    "        'label': y_val_pred,\n",
    "        'protected': protected_attr_val\n",
    "    }), label_names=['label'], protected_attribute_names=['protected'])\n",
    "\n",
    "    # Create BinaryLabelDataset for test data\n",
    "    dataset_test_pred = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=pd.DataFrame({\n",
    "        'label': y_test_pred,\n",
    "        'protected': protected_attr_test\n",
    "    }), label_names=['label'], protected_attribute_names=['protected'])\n",
    "\n",
    "    # Apply EqOddsPostprocessing\n",
    "    eq_odds = EqOddsPostprocessing(unprivileged_groups=[{'protected': 0}],\n",
    "                                   privileged_groups=[{'protected': 1}], seed=seed)\n",
    "\n",
    "    eq_odds = eq_odds.fit(dataset_train_true, dataset_train_pred)\n",
    "    dataset_transf_train_pred = eq_odds.predict(dataset_train_pred)\n",
    "    dataset_transf_val_pred = eq_odds.predict(dataset_val_pred)\n",
    "    dataset_transf_test_pred = eq_odds.predict(dataset_test_pred)\n",
    "\n",
    "    # Extract the adjusted predictions\n",
    "    return dataset_transf_train_pred.labels.ravel(), dataset_transf_val_pred.labels.ravel(), dataset_transf_test_pred.labels.ravel()\n",
    "\n",
    "\n",
    "# build the function for automatically retrieve all metrics\n",
    "def evaluate_metrics(y_true, y_pred, protected):\n",
    "\n",
    "    # calculate the performance metrics\n",
    "    w_precision, w_recall, w_fscore, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    # calculate the fairness metrics\n",
    "    eq_oppor, eq_acc, pred_equal, _, _ = calculate_fairness_measures(y_true, y_pred, protected, unprivileged='m')\n",
    "    \n",
    "    # eq_oppor, eq_acc, fairl_eq_odds, unpriv_stats, priv_stats = calculate_fairness_measures(y_true, y_pred, protected, 'm')\n",
    "    gender_metrics = calculate_gender_performance_measures(y_true, y_pred, protected)\n",
    "\n",
    "    measure_dict = {\n",
    "        \"precision\": w_precision,\n",
    "        \"recall\": w_recall,\n",
    "        \"fscore\": w_fscore,\n",
    "        f\"{gender_metrics[0][0]}_fscore\": gender_metrics[0][3],\n",
    "        f\"{gender_metrics[1][0]}_fscore\": gender_metrics[1][3],\n",
    "        \"eq_oppor\": eq_oppor,\n",
    "        \"eq_acc\": eq_acc,\n",
    "        \"pred_eq\": pred_equal}\n",
    "    return measure_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aaa735-cdce-4f34-91b7-2214eed006e7",
   "metadata": {},
   "source": [
    "# Evaluate unimodal sentence detection models\n",
    "Load in both the normal and keyword filtered model and evaluate them using the test set in order to retrieve the prediction information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872b7c8f-9e99-477c-81e0-dac0e0e9f93d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_path = Path(r\"../DVlog/trained_models\")\n",
    "model_config = Path(r\"../DVlog/model_configs/unimodal/unimodal_mpnet_sent_keyw.cfg\")\n",
    "annotations_file = Path(r\"../DVlog/dataset/dvlog_labels_v2.csv\")\n",
    "save_path = Path(r\"../DVlog/trained_models/eqodds_metrics.csv\")\n",
    "device = torch.device('cpu')\n",
    "\n",
    "random_seeds = [0, 1, 42, 1123, 3407]\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e661ec8b-9aec-469d-9078-1a7fa0434dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: unimodal_mpnet_sent_keyw with seed: 0\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 0\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 0\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 1\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 1\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 1\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 42\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 42\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 42\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 1123\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 1123\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 1123\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 3407\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 3407\n",
      "----------\n",
      "Model: unimodal_mpnet_sent_keyw with seed: 3407\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# load in the config file for the model\n",
    "config = Config().from_disk(model_config)\n",
    "config_dict = process_config(config)\n",
    "\n",
    "# overwrite the annotations_file + data_dir\n",
    "config_dict.annotations_file = annotations_file\n",
    "config_dict.data_dir = Path(\"../DVlog/dataset/sent-embeddings-dataset\")\n",
    "config_dict.encoder1_data_dir = Path(\"../DVlog/dataset/sent-embeddings-dataset\")\n",
    "\n",
    "# setup the model paths\n",
    "model_dir_path = Path(os.path.join(models_path, config_dict.model_name))\n",
    "\n",
    "# \n",
    "for seed in random_seeds:\n",
    "    # set the exact model_path\n",
    "    saved_model_path = Path(os.path.join(model_dir_path, f\"model_{config_dict.model_name}_seed{seed}.pth\"))\n",
    "\n",
    "    # et the seed\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # setup the dataset + loader\n",
    "    train_data = MultimodalEmbeddingsDataset(\"train\", config_dict, to_tensor=True, with_protected=True)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=config_dict.batch_size, shuffle=True)\n",
    "    \n",
    "    val_data = MultimodalEmbeddingsDataset(\"val\", config_dict, to_tensor=True, with_protected=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=config_dict.batch_size, shuffle=True)\n",
    "\n",
    "    test_data = MultimodalEmbeddingsDataset(\"test\", config_dict, to_tensor=True, with_protected=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=config_dict.batch_size, shuffle=True)\n",
    "\n",
    "    # setup the model\n",
    "    saved_model = UnimodalDVlogModel((config_dict.sequence_length, config_dict.encoder1_dim),\n",
    "                                      d_model=config_dict.dim_model, n_heads=config_dict.uni_n_heads, use_std=config_dict.detectlayer_use_std)\n",
    "\n",
    "    # load in the parameters and set the model to evaluation mode\n",
    "    saved_model.load_state_dict(torch.load(saved_model_path, map_location=device))\n",
    "    saved_model.eval()\n",
    "\n",
    "    # run the model on the training set\n",
    "    train_pred, train_y, train_protected, _ = evaluate_model(saved_model, train_dataloader, config_dict, \"m\", False, seed, True)\n",
    "    val_pred, val_y, val_protected, _ = evaluate_model(saved_model, val_dataloader, config_dict, \"m\", False, seed, True)\n",
    "    test_pred, test_y, test_protected, _ = evaluate_model(saved_model, test_dataloader, config_dict, \"m\", False, seed, True)\n",
    "\n",
    "    # reshape all predictions, ground truths, and protected\n",
    "    train_pred = np.argmax(train_pred, axis=1)\n",
    "    train_y = np.argmax(train_y, axis=1)\n",
    "    train_protected_float = np.where(train_protected == \"m\", 0, 1)\n",
    "    \n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    val_y = np.argmax(val_y, axis=1)\n",
    "    val_protected_float = np.where(val_protected == \"m\", 0, 1)\n",
    "\n",
    "    test_pred = np.argmax(test_pred, axis=1)\n",
    "    test_y = np.argmax(test_y, axis=1)\n",
    "    test_protected_float = np.where(test_protected == \"m\", 0, 1)\n",
    "\n",
    "    # run the post-processing code\n",
    "    new_train_preds, new_val_preds, new_test_preds = apply_eqodds(train_y, train_pred, val_pred, test_pred, train_protected_float, val_protected_float, test_protected_float, seed)\n",
    "\n",
    "    # evaluate the models\n",
    "    eval_dict_train = evaluate_metrics(train_y, new_train_preds, train_protected)\n",
    "    eval_dict_val = evaluate_metrics(val_y, new_val_preds, val_protected)\n",
    "    eval_dict_test = evaluate_metrics(test_y, new_test_preds, test_protected)\n",
    "\n",
    "    results.append((\"eqodds\", \"train\", seed, eval_dict_train))\n",
    "    results.append((\"eqodds\", \"val\", seed, eval_dict_val))\n",
    "    results.append((\"eqodds\", \"test\", seed, eval_dict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f9212a-afd8-4791-86b9-873682c5211d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_m</th>\n",
       "      <th>f1_f</th>\n",
       "      <th>eq opportunity</th>\n",
       "      <th>Eq accuracy</th>\n",
       "      <th>pred equality</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <th>eqodds</th>\n",
       "      <td>914.6</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>0.8806</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <th>eqodds</th>\n",
       "      <td>914.6</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <th>eqodds</th>\n",
       "      <td>914.6</td>\n",
       "      <td>0.9292</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 seed  precision  recall      f1    f1_m    f1_f  \\\n",
       "dataset name                                                       \n",
       "test    eqodds  914.6     0.9270  0.9258  0.9256  0.8806  0.9500   \n",
       "train   eqodds  914.6     0.9762  0.9760  0.9758  0.9770  0.9752   \n",
       "val     eqodds  914.6     0.9292  0.9286  0.9286  0.9310  0.9270   \n",
       "\n",
       "                eq opportunity  Eq accuracy  pred equality  \n",
       "dataset name                                                \n",
       "test    eqodds           0.896        0.928          1.488  \n",
       "train   eqodds           1.000        1.000          0.966  \n",
       "val     eqodds           0.988        1.002          1.028  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data into a structured format\n",
    "extracted_data = []\n",
    "for name, dataset, seed, result in results:\n",
    "    data = {\n",
    "        \"name\": name,\n",
    "        \"dataset\": dataset,\n",
    "        \"seed\": seed,\n",
    "        \"precision\": np.round(result[\"precision\"], 3),\n",
    "        \"recall\": np.round(result[\"recall\"], 3),\n",
    "        \"f1\": np.round(result[\"fscore\"], 3),\n",
    "        \"f1_m\": np.round(result[\"m_fscore\"], 3),\n",
    "        \"f1_f\": np.round(result[\"f_fscore\"], 3),\n",
    "        \"eq opportunity\": np.round(result[\"eq_oppor\"], 2),\n",
    "        \"Eq accuracy\": np.round(result[\"eq_acc\"], 2),\n",
    "        \"pred equality\": np.round(result[\"pred_eq\"], 2)\n",
    "    }\n",
    "    extracted_data.append(data)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame and display it\n",
    "df = pd.DataFrame(extracted_data)\n",
    "df.groupby([\"dataset\", \"name\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac93fac1-7158-40aa-a36d-cf813d728caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.to_csv(save_path, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
