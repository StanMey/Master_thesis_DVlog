{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028dbc52-5781-48f0-bb4a-3b83b1dc5358",
   "metadata": {},
   "source": [
    "# Post-hoc model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d09900b5-8095-4da8-9bbb-04bb9d3fa549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from confection import Config\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# update the path so we can directly import code from the DVlog\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"DVlog\"))))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, \"DVlog\")))\n",
    "\n",
    "from DVlog.evaluate import evaluate_model\n",
    "from DVlog.models.model import UnimodalDVlogModel\n",
    "from DVlog.utils.dataloaders import MultimodalEmbeddingsDataset\n",
    "from DVlog.utils.metrics import calculate_performance_measures, calculate_gender_performance_measures\n",
    "from DVlog.utils.util import ConfigDict, validate_config, process_config, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a601c-4d8b-488c-8132-f6664d444258",
   "metadata": {},
   "source": [
    "## load in the annotations file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5bc12c-0c64-49bc-84db-ec2de790aca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_dir = Path(r\"../data/dvlog_text\")\n",
    "annotations_file = Path(r\"../DVlog/dataset/dvlog_labels_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6be1f1-3251-4c46-978f-920016df54b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>label</th>\n",
       "      <th>gender</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    video_id  label gender dataset\n",
       "1          1      1      f    test\n",
       "4          4      1      f    test\n",
       "6          6      1      f    test\n",
       "12        13      1      f    test\n",
       "13        15      1      f    test"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the annotation labels\n",
    "df_annotations = pd.read_csv(annotations_file)\n",
    "\n",
    "# select the test set\n",
    "df_annotations = df_annotations[df_annotations[\"dataset\"] == \"test\"]\n",
    "df_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59344a7-0795-4631-93a9-d0138b253a4a",
   "metadata": {},
   "source": [
    "## Evaluate detection models\n",
    "Load in both the normal and keyword filtered model and evaluate them using the test set in order to retrieve the prediction information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66004e62-706b-4292-9fdd-b1fc7f80a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = Path(r\"../DVlog/trained_models\")\n",
    "normal_model_config = Path(r\"../DVlog/model_configs/unimodal/unimodal_mpnet_sent_normal.cfg\")\n",
    "keyw_removed_model_config = Path(r\"../DVlog/model_configs/unimodal/unimodal_mpnet_sent_keyw.cfg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3500c91-6c24-4d85-b115-2aaf32afa1cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f591ee2a-88a8-4678-9f6c-ce966b007de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset\\\\sent-embeddings-dataset\\\\250\\\\sent_mpnet_normal.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39mconfig_dict\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# evaluate both models and retrieve the raw prediction information\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m norm_pred, norm_truth, norm_protected, norm_video_ids \u001b[38;5;241m=\u001b[39m evaluate_model(saved_model, test_dataloader, config_dict,\n\u001b[0;32m     30\u001b[0m                                                                        unpriv_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, get_raw_preds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# retrieve the evaluation metrics to check the model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m _, w_precision, w_recall, w_fscore, _ \u001b[38;5;241m=\u001b[39m calculate_performance_measures(y_labels, predictions)\n",
      "File \u001b[1;32m~\\Documents\\Master\\thesis\\Master_thesis_DVlog\\DVlog\\evaluate.py:118\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_dataloader, config_dict, unpriv_feature, verbose, get_raw_preds)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Disable gradient computation and reduce memory consumption.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, vdata \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[0;32m    119\u001b[0m         \n\u001b[0;32m    120\u001b[0m         \u001b[38;5;66;03m# get the inputs\u001b[39;00m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m config_dict\u001b[38;5;241m.\u001b[39mn_modalities \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    122\u001b[0m             \u001b[38;5;66;03m# only one modality so unpack the set\u001b[39;00m\n\u001b[0;32m    123\u001b[0m             v_inputs, vlabels, v_protected, v_video_id \u001b[38;5;241m=\u001b[39m vdata\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dvlog_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dvlog_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dvlog_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dvlog_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\Documents\\Master\\thesis\\Master_thesis_DVlog\\DVlog\\utils\\dataloaders.py:47\u001b[0m, in \u001b[0;36mMultimodalEmbeddingsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m protected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# load in the first embedding\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve_embedding(video_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder1_data_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder1_feature_name), )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_modalities \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# load in the additional embedding\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39membeddings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve_embedding(video_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder2_data_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder2_feature_name))\n",
      "File \u001b[1;32m~\\Documents\\Master\\thesis\\Master_thesis_DVlog\\DVlog\\utils\\dataloaders.py:84\u001b[0m, in \u001b[0;36mMultimodalEmbeddingsDataset._retrieve_embedding\u001b[1;34m(self, idx, data_dir, feature_name)\u001b[0m\n\u001b[0;32m     81\u001b[0m feature_name \u001b[38;5;241m=\u001b[39m feature_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m embeddings_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;28mstr\u001b[39m(idx), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(embeddings_path)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dvlog_env\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset\\\\sent-embeddings-dataset\\\\250\\\\sent_mpnet_normal.npy'"
     ]
    }
   ],
   "source": [
    "# load in the first config dict and evaluate the normal model\n",
    "config = Config().from_disk(normal_model_config)\n",
    "config_dict = process_config(config)\n",
    "\n",
    "# overwrite the annotations_file + data_dir\n",
    "config_dict.annotations_file = annotations_file\n",
    "config_dict.data_dir = Path(\"../DVlog/dataset/sent-embeddings-dataset\")\n",
    "\n",
    "# setup the model paths\n",
    "model_dir_path = Path(os.path.join(models_path, config_dict.model_name))\n",
    "saved_model_path = Path(os.path.join(model_dir_path, f\"model_{config_dict.model_name}.pth\"))\n",
    "\n",
    "# set the seed\n",
    "set_seed(42)\n",
    "\n",
    "# setup the original model\n",
    "saved_model = UnimodalDVlogModel((config_dict.sequence_length, config_dict.encoder1_dim),\n",
    "                                   d_model=config_dict.dim_model, n_heads=config_dict.uni_n_heads, use_std=config_dict.detectlayer_use_std)\n",
    "\n",
    "# load in the parameters and set the model to evaluation mode\n",
    "saved_model.load_state_dict(torch.load(saved_model_path))\n",
    "saved_model.eval()\n",
    "\n",
    "# setup the dataset + loader\n",
    "test_data = MultimodalEmbeddingsDataset(\"test\", config_dict, to_tensor=True, with_protected=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=config_dict.batch_size, shuffle=True)\n",
    "\n",
    "# evaluate both models and retrieve the raw prediction information\n",
    "norm_pred, norm_truth, norm_protected, norm_video_ids = evaluate_model(saved_model, test_dataloader, config_dict,\n",
    "                                                                       unpriv_feature=\"m\", verbose=False, get_raw_preds=True)\n",
    "\n",
    "# retrieve the evaluation metrics to check the model\n",
    "_, w_precision, w_recall, w_fscore, _ = calculate_performance_measures(y_labels, predictions)\n",
    "gender_metrics = calculate_gender_performance_measures(y_labels, predictions, protected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442fc39-a89f-46d7-8bac-bc0ed27fa758",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyw_pred, keyw_truth, keyw_protected, keyw_video_ids = evaluate(normal_model_config, trained_model_dir, 0,\n",
    "                                                                 unpriv_feature, verbose=False, get_raw_predictions=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
