{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5720c5a4-77c2-47ec-895f-b1265637865a",
   "metadata": {},
   "source": [
    "# Notebook for using Sentence embeddings\n",
    "\n",
    "### sources\n",
    "- https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    "- https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5740bb8e-3cce-4905-aaac-aec8ea35b893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import os\n",
    "import operator\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b8ed2-c556-4735-9074-0f1ce05fb126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3ffa28-06d0-4e85-b3fe-383ba8d99ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_dir = Path(r\"../data/dvlog_text\")\n",
    "annotations_file = Path(r\"../DVlog/dataset/dvlog_labels_v2.csv\")\n",
    "embeddings_save_folder = Path(r\"E:/master/data/sent-embeddings-dataset\")\n",
    "\n",
    "# load in the annotation labels\n",
    "df_annotations = pd.read_csv(annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aef59ae-a183-4176-b4a7-2c37f45823e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the synonym keywords\n",
    "depr_synonyms_file1 = Path(r\"../data/depression_synonyms_gizem.json\")\n",
    "depr_synonyms_file2 = Path(r\"../data/depression_synonyms_paper.json\")\n",
    "\n",
    "# load in the files and combine them into a single list with keywords\n",
    "with open(depr_synonyms_file1) as current_file:\n",
    "    depri_synonyms = list(json.loads(current_file.read()).get(\"depression\"))\n",
    "\n",
    "with open(depr_synonyms_file2) as current_file:\n",
    "    depri2 = list(json.loads(current_file.read()).get(\"depression\"))\n",
    "\n",
    "# combine them and remove duplicates\n",
    "depri_synonyms.extend(depri2)\n",
    "depri_synonyms = list(set(depri_synonyms))\n",
    "len(depri_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e08e6b-8b3a-4311-a3e2-d7a72d9320cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop over each text file and extract the text\n",
    "text_ref_dict = {}\n",
    "for json_file in os.listdir(json_dir):\n",
    "    \n",
    "    # get the video_id and setup the path to the file\n",
    "    video_id = int(json_file.split(\"_\")[0])\n",
    "    json_path = os.path.join(json_dir, json_file)\n",
    "    \n",
    "    with open(json_path) as current_file:\n",
    "        json_dict = json.loads(current_file.read())\n",
    "\n",
    "    text_ref_dict[video_id] = {\n",
    "        \"text\": json_dict[\"text\"],\n",
    "        \"text_segments\": [x.get(\"text\") for x in json_dict[\"segments\"]],\n",
    "        \"words\": [(x.get(\"text\"), x.get(\"start\"), x.get(\"end\")) for x in list(chain.from_iterable([x.get(\"words\") for x in json_dict[\"segments\"]]))]\n",
    "    }\n",
    "\n",
    "# put the annotations back into the dataframe\n",
    "# df_annotations[\"text\"] = df_annotations[\"video_id\"].apply(lambda x: text_ref_dict.get(x).get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d7de5a-a183-487d-8c3f-fbc4363603e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str, unicode_pattern: str = \"NFKD\") -> str:\n",
    "    text = text.replace(\"\\n\", \"\").strip()  # Remove newlines and trailing whitespace\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.digits))  # Remove all numbers with lookup table\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation with lookup table\n",
    "    text = \" \".join(text.split()) # Remove excess whitespace in between words\n",
    "    text = unicodedata.normalize(unicode_pattern, text)  # Strip accents from characters\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74caa9b5-69a6-4e49-a6c3-a4d971f47100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sent_embeddings(model, text_dict: dict, df: pd.DataFrame, feature_name: str,\n",
    "                             save_folder: Path, depri_keywords: list):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        # get the texts\n",
    "        video_id = row.video_id\n",
    "        texts = text_dict.get(video_id).get(\"text_segments\")\n",
    "\n",
    "        # clean the texts\n",
    "        texts = [clean_text(text) for text in texts if len(clean_text(text).split()) > 1]\n",
    "\n",
    "        if depri_keywords:\n",
    "            sentences = []\n",
    "            # check for each keyword on each sentence\n",
    "            for sentence in texts:\n",
    "                skip_sentence = False\n",
    "                for keyw in depri_keywords:\n",
    "                    if keyw in sentence:\n",
    "                        skip_sentence = True\n",
    "                        break\n",
    "    \n",
    "                if not skip_sentence:\n",
    "                    sentences.append(sentence)\n",
    "\n",
    "        else:\n",
    "            sentences = texts    \n",
    "\n",
    "        # put it through the model\n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # save the embedding\n",
    "        subject_output_path = os.path.join(save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "    \n",
    "        np.save(os.path.join(subject_output_path, f\"{feature_name}.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a91296-37df-496a-a484-216e65ea8db2",
   "metadata": {},
   "source": [
    "## SBERT (all-mpnet-base-v2)\n",
    "- https://sbert.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f2d284-6bde-48ec-82a4-f419652603b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "run_mpnet_sbert = True\n",
    "\n",
    "if run_mpnet_sbert:\n",
    "    model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bf26cd-cd4f-4179-8f79-5b3e3bb171b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature_normal_name = \"sent_mpnet_normal\"\n",
    "output_feature_keyw_name = \"sent_mpnet_keyw\"\n",
    "\n",
    "# run the sentence embedding process\n",
    "if run_mpnet_sbert:\n",
    "    # get the normal embeddings\n",
    "    retrieve_sent_embeddings(model, text_ref_dict, df_annotations, output_feature_normal_name,\n",
    "                             embeddings_save_folder, [])\n",
    "\n",
    "    # get the filtered embeddings\n",
    "    retrieve_sent_embeddings(model, text_ref_dict, df_annotations, output_feature_keyw_name,\n",
    "                             embeddings_save_folder, depri_synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf019d-2dc5-49d3-8c67-81ab2d128582",
   "metadata": {},
   "source": [
    "### spaCy experiment\n",
    "- https://stackoverflow.com/questions/46290313/how-to-break-up-document-by-sentences-with-spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4233d8d-eb76-4992-aa37-a5c21e893822",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature_name = \"sent_mpnet_spacy_normal\"\n",
    "\n",
    "if run_mpnet_sbert:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    for index, row in df_annotations.iterrows():\n",
    "        # get the texts\n",
    "        video_id = row.video_id\n",
    "        text = text_ref_dict.get(video_id).get(\"text\")\n",
    "\n",
    "        # extract the spaCy made sentence embeddings\n",
    "        with nlp.select_pipes(enable=['tok2vec', \"parser\", \"senter\"]):\n",
    "            doc = nlp(text)\n",
    "\n",
    "        sentences = [clean_text(sent.text) for sent in doc.sents if len(clean_text(sent.text).split()) > 1]\n",
    "\n",
    "        # put it through the model\n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # save the embedding\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "    \n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27cd20ab-228f-4b94-a601-ef650a51e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature_name = \"sent_mpnet_spacy_keyw\"\n",
    "\n",
    "if run_mpnet_sbert:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    for index, row in df_annotations.iterrows():\n",
    "        # get the texts\n",
    "        video_id = row.video_id\n",
    "        text = text_ref_dict.get(video_id).get(\"text\")\n",
    "\n",
    "        # extract the spaCy made sentence embeddings\n",
    "        with nlp.select_pipes(enable=['tok2vec', \"parser\", \"senter\"]):\n",
    "            doc = nlp(text)\n",
    "\n",
    "        texts = [clean_text(sent.text) for sent in doc.sents if len(clean_text(sent.text).split()) > 1]\n",
    "\n",
    "        sentences = []\n",
    "        # check for each keyword on each sentence\n",
    "        for sentence in texts:\n",
    "            skip_sentence = False\n",
    "            for keyw in depri_synonyms:\n",
    "                if keyw in sentence:\n",
    "                    skip_sentence = True\n",
    "                    break\n",
    "\n",
    "            if not skip_sentence:\n",
    "                sentences.append(sentence)\n",
    "\n",
    "        # put it through the model\n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # save the embedding\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "    \n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f9d768-f96a-496c-93b9-2c2493528e78",
   "metadata": {},
   "source": [
    "## SBERT (all-MiniLM-L12-v2)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a74482c-ee9b-4a16-b9a3-93a863532759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 175kB/s]\n",
      "config_sentence_transformers.json: 100%|██████████████████████████████████████████████████████| 116/116 [00:00<?, ?B/s]\n",
      "README.md: 100%|██████████████████████████████████████████████████████████████████| 10.7k/10.7k [00:00<00:00, 10.7MB/s]\n",
      "sentence_bert_config.json: 100%|████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 53.4kB/s]\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 615/615 [00:00<?, ?B/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 133M/133M [00:11<00:00, 11.2MB/s]\n",
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████| 352/352 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 1.35MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 1.88MB/s]\n",
      "special_tokens_map.json: 100%|████████████████████████████████████████████████████████████████| 112/112 [00:00<?, ?B/s]\n",
      "1_Pooling/config.json: 100%|██████████████████████████████████████████████████████████████████| 190/190 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"all-MiniLM-L12-v2\"\n",
    "run_minilm_sbert = True\n",
    "\n",
    "if run_minilm_sbert:\n",
    "    model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2aaa982-bc78-4ed1-a82c-78bae1f97e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature_normal_name = \"sent_minilm_normal\"\n",
    "output_feature_keyw_name = \"sent_minilm_keyw\"\n",
    "\n",
    "# run the sentence embedding process\n",
    "if run_minilm_sbert:\n",
    "    # get the normal embeddings\n",
    "    retrieve_sent_embeddings(model, text_ref_dict, df_annotations, output_feature_normal_name,\n",
    "                             embeddings_save_folder, [])\n",
    "\n",
    "    # get the filtered embeddings\n",
    "    retrieve_sent_embeddings(model, text_ref_dict, df_annotations, output_feature_keyw_name,\n",
    "                             embeddings_save_folder, depri_synonyms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
