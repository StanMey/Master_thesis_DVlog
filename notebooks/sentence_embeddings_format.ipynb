{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5720c5a4-77c2-47ec-895f-b1265637865a",
   "metadata": {},
   "source": [
    "# Notebook for using Sentence embeddings\n",
    "\n",
    "### sources\n",
    "- https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    "- https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5740bb8e-3cce-4905-aaac-aec8ea35b893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import os\n",
    "import operator\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b8ed2-c556-4735-9074-0f1ce05fb126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3ffa28-06d0-4e85-b3fe-383ba8d99ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_dir = Path(r\"../data/dvlog_text\")\n",
    "annotations_file = Path(r\"../DVlog/dataset/dvlog_labels_v2.csv\")\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "\n",
    "# load in the annotation labels\n",
    "df_annotations = pd.read_csv(annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e08e6b-8b3a-4311-a3e2-d7a72d9320cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop over each text file and extract the text\n",
    "text_ref_dict = {}\n",
    "for json_file in os.listdir(json_dir):\n",
    "    \n",
    "    # get the video_id and setup the path to the file\n",
    "    video_id = int(json_file.split(\"_\")[0])\n",
    "    json_path = os.path.join(json_dir, json_file)\n",
    "    \n",
    "    with open(json_path) as current_file:\n",
    "        json_dict = json.loads(current_file.read())\n",
    "\n",
    "    text_ref_dict[video_id] = {\n",
    "        \"text\": json_dict[\"text\"],\n",
    "        \"text_segments\": [x.get(\"text\") for x in json_dict[\"segments\"]],\n",
    "        \"words\": [(x.get(\"text\"), x.get(\"start\"), x.get(\"end\")) for x in list(chain.from_iterable([x.get(\"words\") for x in json_dict[\"segments\"]]))]\n",
    "    }\n",
    "\n",
    "# put the annotations back into the dataframe\n",
    "df_annotations[\"text\"] = df_annotations[\"video_id\"].apply(lambda x: text_ref_dict.get(x).get(\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0b0e9-aca9-4efb-ac5e-5d9e71b8d7b4",
   "metadata": {},
   "source": [
    "# load in the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126fcd4-7801-4c19-9863-c859264fe456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ca238-bbe7-432a-a6a8-5c19a4b8b077",
   "metadata": {},
   "source": [
    "## Clinical-BERT sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ef3fed-e5ba-4173-a93b-90c2f9df6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix misspelled words\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispell_dict = {\n",
    "    'colour':'color',\n",
    "    'didnt':'did not',\n",
    "    'doesnt':'does not',\n",
    "    'isnt':'is not',\n",
    "    'hasnt': 'has not',\n",
    "    'shouldnt':'should not',\n",
    "    'wasnt': 'was not',\n",
    "    'instagram': 'social medium',\n",
    "    'whatsapp': 'social medium',\n",
    "    'snapchat': 'social medium'\n",
    "}\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "# we clean the text on punctuation, newlines and trailing whitespace.\n",
    "# we also remove \n",
    "def clean_text(text: str, mispelled_func: Callable) -> str:\n",
    "    text = text.replace(\"\\n\", \"\").strip()  # Remove newlines and trailing whitespace\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove puctuation with lookup table\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # clean the numbers\n",
    "    text = re.sub('[0-9]{5,}', '#####', text)\n",
    "    text = re.sub('[0-9]{4}', '####', text)\n",
    "    text = re.sub('[0-9]{3}', '###', text)\n",
    "    text = re.sub('[0-9]{2}', '##', text)\n",
    "\n",
    "    # fix misspellings\n",
    "    text = mispelled_func(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63e341c-4182-4f46-9b17-6d5226147de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embeddings without extra keywords\n",
    "keyw_to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_word2vec:\n",
    "    # retrieve the embedding features with averaged unknown vectors\n",
    "    extract_text_segments(df_annotations, text_ref_dict, w2v_embeddings_index,\n",
    "                          keyw_to_remove, embeddings_save_folder, \"w2v_seconds_normal_avg\")\n",
    "\n",
    "    # retrieve the embedding features with unknown zero-vectors\n",
    "    extract_text_segments(df_annotations, text_ref_dict, w2v_embeddings_index,\n",
    "                          keyw_to_remove, embeddings_save_folder, \"w2v_seconds_normal_zero\", use_avg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59e37db-a721-40aa-b3c1-4eace59298f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embedding with extra keywords\n",
    "depression_keywords = [\"depression\", \"depressive\", \"antidepressant\", \"depressed\", \"anxiety\", \"psychiatrist\", \"ptsd\"]\n",
    "depression_keywords.extend(keyw_to_remove)\n",
    "\n",
    "if run_word2vec:\n",
    "    extract_text_segments(df_annotations, text_ref_dict, w2v_embeddings_index,\n",
    "                          depression_keywords, embeddings_save_folder, \"w2v_seconds_keyw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e42b7d84-08ae-48dc-8859-78b90d86f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averaged text embeddings\n",
    "output_feature_name = \"w2v_seq_avg\"\n",
    "\n",
    "if run_word2vec:\n",
    "    for index, row in df_annotations.iterrows():\n",
    "    \n",
    "        video_id = row.video_id\n",
    "        final_embeddings = []\n",
    "    \n",
    "        texts = text_ref_dict.get(video_id).get(\"text_segments\")\n",
    "        for text in texts:\n",
    "            # clean up the words\n",
    "            cleaned_text = clean_text(text, replace_typical_misspell).split()\n",
    "    \n",
    "            # remove some of the keywords\n",
    "            to_remove = ['a', 'to', 'of', 'and']\n",
    "            cleaned_text = [word for word in cleaned_text if not word in to_remove]\n",
    "    \n",
    "            # disregard sentences with single words\n",
    "            if len(cleaned_text) <= 1:\n",
    "                continue\n",
    "            else:\n",
    "                # get the embedding\n",
    "                embedding = w2v_embeddings_index.get_mean_vector(cleaned_text)\n",
    "                final_embeddings.append(embedding)\n",
    "                \n",
    "        # save the embedding\n",
    "        final_embeddings = np.array(final_embeddings)\n",
    "\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffebe11-9daa-4557-ae91-ed63a3c0ee4f",
   "metadata": {},
   "source": [
    "## BioWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be86f2df-df42-4822-9e4b-8dca841041ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_biowordvec = True\n",
    "\n",
    "if run_biowordvec:\n",
    "    biowordvec_path = Path(r\"E:/master/embedding_models/bio_embedding_extrinsic\")\n",
    "    assert os.path.exists(biowordvec_path), \"embedding model not found\"\n",
    "\n",
    "    bio_embeddings_index = KeyedVectors.load_word2vec_format(biowordvec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e02e707-52fd-469f-9416-2f79f43050dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 20988/20988 [00:00<00:00, 277681.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 88.10% of vocab\n",
      "Found embeddings for  98.57% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, bio_embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a8594c-c4c1-4124-8bc6-c3744e9279c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and save the embeddings#\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "\n",
    "# extract the embeddings without extra keywords\n",
    "keyw_to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_biowordvec:\n",
    "    # retrieve the embedding features with averaged unknown vectors\n",
    "    extract_text_segments(df_annotations, text_ref_dict, bio_embeddings_index,\n",
    "                          keyw_to_remove, embeddings_save_folder, \"biow_seconds_normal_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27dcd530-c8b8-4435-bb05-52d62cdec4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averaged text embeddings\n",
    "output_feature_name = \"biow_seq_avg\"\n",
    "\n",
    "if run_biowordvec:\n",
    "    for index, row in df_annotations.iterrows():\n",
    "    \n",
    "        video_id = row.video_id\n",
    "        final_embeddings = []\n",
    "    \n",
    "        texts = text_ref_dict.get(video_id).get(\"text_segments\")\n",
    "        for text in texts:\n",
    "            # clean up the words\n",
    "            cleaned_text = clean_text(text, replace_typical_misspell).split()\n",
    "    \n",
    "            # remove some of the keywords\n",
    "            to_remove = ['a', 'to', 'of', 'and']\n",
    "            cleaned_text = [word for word in cleaned_text if not word in to_remove]\n",
    "    \n",
    "            # disregard sentences with single words\n",
    "            if len(cleaned_text) <= 1:\n",
    "                continue\n",
    "            else:\n",
    "                # get the embedding\n",
    "                embedding = bio_embeddings_index.get_mean_vector(cleaned_text)\n",
    "                final_embeddings.append(embedding)\n",
    "                \n",
    "        # save the embedding\n",
    "        final_embeddings = np.array(final_embeddings)\n",
    "\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), final_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
