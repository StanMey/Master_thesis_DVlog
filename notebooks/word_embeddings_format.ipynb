{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5720c5a4-77c2-47ec-895f-b1265637865a",
   "metadata": {},
   "source": [
    "# Notebook for using Word embeddings\n",
    "\n",
    "### sources\n",
    "- https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "- https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5740bb8e-3cce-4905-aaac-aec8ea35b893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim.downloader\n",
    "import json\n",
    "import os\n",
    "import operator\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b8ed2-c556-4735-9074-0f1ce05fb126",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3ffa28-06d0-4e85-b3fe-383ba8d99ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_dir = Path(r\"../data/dvlog_text\")\n",
    "annotations_file = Path(r\"../DVlog/dataset/dvlog_labels_v2.csv\")\n",
    "\n",
    "# load in the annotation labels\n",
    "df_annotations = pd.read_csv(annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a359df8-db39-476b-b197-4dff19420373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the synonym keywords\n",
    "depr_synonyms_file1 = Path(r\"../data/depression_synonyms_gizem.json\")\n",
    "depr_synonyms_file2 = Path(r\"../data/depression_synonyms_paper.json\")\n",
    "\n",
    "# load in the files and combine them into a single list with keywords\n",
    "with open(depr_synonyms_file1) as current_file:\n",
    "    depri_synonyms = list(json.loads(current_file.read()).get(\"depression\"))\n",
    "\n",
    "with open(depr_synonyms_file2) as current_file:\n",
    "    depri2 = list(json.loads(current_file.read()).get(\"depression\"))\n",
    "\n",
    "# combine them and remove duplicates\n",
    "depri_synonyms.extend(depri2)\n",
    "depri_synonyms = list(set(depri_synonyms))\n",
    "len(depri_synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0b0e9-aca9-4efb-ac5e-5d9e71b8d7b4",
   "metadata": {},
   "source": [
    "# setting up some functions to check the coverage of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ea4a59-74f6-4ee2-bf4d-85f8afad409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose = True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(int)\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f08747-cdcf-497d-b677-183283746686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ca238-bbe7-432a-a6a8-5c19a4b8b077",
   "metadata": {},
   "source": [
    "## Word2vec (Google-news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b6e749-2885-4793-8056-47a4cc88a899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_word2vec = True\n",
    "\n",
    "if run_word2vec:\n",
    "    google_news_path = Path(r\"E:/master/embedding_models/GoogleNews-vectors-negative300.bin\")\n",
    "    assert os.path.exists(google_news_path), \"embedding model not found\"\n",
    "\n",
    "    w2v_embeddings_index = KeyedVectors.load_word2vec_format(google_news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506b5401-f072-40b1-a898-52d9a58b9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each text file and extract the text\n",
    "text_ref_dict = {}\n",
    "for json_file in os.listdir(json_dir):\n",
    "    \n",
    "    # get the video_id and setup the path to the file\n",
    "    video_id = int(json_file.split(\"_\")[0])\n",
    "    json_path = os.path.join(json_dir, json_file)\n",
    "    \n",
    "    with open(json_path) as current_file:\n",
    "        json_dict = json.loads(current_file.read())\n",
    "\n",
    "    text_ref_dict[video_id] = {\n",
    "        \"text\": json_dict[\"text\"],\n",
    "        \"text_segments\": [x.get(\"text\") for x in json_dict[\"segments\"]],\n",
    "        \"words\": [(x.get(\"text\"), x.get(\"start\"), x.get(\"end\")) for x in list(chain.from_iterable([x.get(\"words\") for x in json_dict[\"segments\"]]))]\n",
    "    }\n",
    "\n",
    "# put the annotations back into the dataframe\n",
    "df_annotations[\"text\"] = df_annotations[\"video_id\"].apply(lambda x: text_ref_dict.get(x).get(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ef3fed-e5ba-4173-a93b-90c2f9df6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix misspelled words\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispell_dict = {\n",
    "    'colour':'color',\n",
    "    'didnt':'did not',\n",
    "    'doesnt':'does not',\n",
    "    'isnt':'is not',\n",
    "    'hasnt': 'has not',\n",
    "    'shouldnt':'should not',\n",
    "    'wasnt': 'was not',\n",
    "    'instagram': 'social medium',\n",
    "    'whatsapp': 'social medium',\n",
    "    'snapchat': 'social medium'\n",
    "}\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "# we clean the text on punctuation, newlines and trailing whitespace.\n",
    "# we also remove \n",
    "def clean_text(text: str, mispelled_func: Callable) -> str:\n",
    "    text = text.replace(\"\\n\", \"\").strip()  # Remove newlines and trailing whitespace\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove puctuation with lookup table\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # clean the numbers\n",
    "    text = re.sub('[0-9]{5,}', '#####', text)\n",
    "    text = re.sub('[0-9]{4}', '####', text)\n",
    "    text = re.sub('[0-9]{3}', '###', text)\n",
    "    text = re.sub('[0-9]{2}', '##', text)\n",
    "\n",
    "    # fix misspellings\n",
    "    text = mispelled_func(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef80c065-93b0-4e00-b4f8-3307bed0661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 827/827 [00:00<00:00, 6484.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 827/827 [00:00<00:00, 5090.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# select the sentences and build the vocab\n",
    "df_annotations[\"cleaned_text\"] = df_annotations[\"text\"].apply(lambda x: clean_text(x, replace_typical_misspell))\n",
    "sentences = df_annotations[\"cleaned_text\"].apply(lambda x: x.split())\n",
    "to_remove = ['a', 'to', 'of', 'and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "885b96de-16f1-4d79-ad44-a4ac0edfc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 20988/20988 [00:00<00:00, 540844.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 87.96% of vocab\n",
      "Found embeddings for  99.60% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, w2v_embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e40c83-381d-40a4-95ac-2c4c6e5dd640",
   "metadata": {},
   "source": [
    "### Build the word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4709c09-b261-44e1-8837-c9d5f913b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_segments(df, text_ref_dict, embeddings_index, keyword_removal, features_output_path: Path, output_feature_name: str, use_avg: bool = True):\n",
    "    # function to get the per-seconds embeddings where we only look at the start time of the word\n",
    "    assert os.path.exists(features_output_path), \"output directory does not exist\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        video_id = row.video_id\n",
    "        words_list = text_ref_dict.get(video_id).get(\"words\")\n",
    "    \n",
    "        start_time = math.floor(words_list[0][1])\n",
    "        end_time = math.floor(words_list[-1][1])\n",
    "        # print(video_id, words_list[0], words_list[-1], start_time, end_time)\n",
    "    \n",
    "        # set up the dictionary to store the word embeddings\n",
    "        embed_dict = {}\n",
    "    \n",
    "        for i in range(start_time, end_time + 1):\n",
    "            embed_dict[i] = []\n",
    "    \n",
    "        # go over all words, clean them and put them in the dictionary\n",
    "        for word, start_time, end_time in words_list:\n",
    "    \n",
    "            word_i = math.floor(start_time)\n",
    "            cleaned_word = clean_text(word, replace_typical_misspell)\n",
    "\n",
    "            # remove keywords\n",
    "            if cleaned_word in keyword_removal:\n",
    "                continue\n",
    "    \n",
    "            if len(cleaned_word.split(\" \")) == 2:\n",
    "                # some words when cleaned up are now two words so handle them appropriatly\n",
    "                cleaned1, cleaned2 = cleaned_word.split(\" \")\n",
    "                if embeddings_index.has_index_for(cleaned1):\n",
    "                    embed_dict[word_i].append(embeddings_index[cleaned1])\n",
    "    \n",
    "                if embeddings_index.has_index_for(cleaned2):\n",
    "                    embed_dict[word_i].append(embeddings_index[cleaned2])\n",
    "    \n",
    "            else:\n",
    "                if embeddings_index.has_index_for(cleaned_word):\n",
    "                    # check if we have an index for the word (otherwise we ignore it)\n",
    "                    embed_dict[word_i].append(embeddings_index[cleaned_word])\n",
    "    \n",
    "        # now we want to average all embeddings on all times\n",
    "        embeddings = sorted([(k, np.array(v)) for k,v in embed_dict.items()], key=lambda a: a[0])\n",
    "        embeddings = [np.mean(v, axis=0) for k,v in embeddings]\n",
    "\n",
    "        if use_avg:\n",
    "            # handle the vectors for which no information is known by averaging the neighboring vectors\n",
    "            final_embeddings = [np.nan for x in embeddings]\n",
    "            last_true_embed = np.nan\n",
    "        \n",
    "            for i, embed in enumerate(embeddings):\n",
    "        \n",
    "                if not np.isnan(final_embeddings[i]).any():\n",
    "                    # check if this is already filled in (for an averaged vector this is the case)\n",
    "                    continue\n",
    "        \n",
    "                else:\n",
    "                    if not np.isnan(embed).any():\n",
    "                        # we have a mean embedding so just store it and continue\n",
    "                        final_embeddings[i] = embeddings[i]\n",
    "                        last_true_embed = embeddings[i]\n",
    "        \n",
    "                    else:\n",
    "                        # we don't have an embedding so we have to average with one\n",
    "                        found_end_embed = False\n",
    "                        curr_index = i + 1\n",
    "        \n",
    "                        while curr_index < len(embeddings):\n",
    "        \n",
    "                            if not np.isnan(embeddings[curr_index]).any():\n",
    "                                # we found the end of the gap\n",
    "                                # print(last_true_embed.shape, embeddings[curr_index].shape)\n",
    "                                if np.isnan(last_true_embed).any():\n",
    "                                    # when the first token is None, we take the first found embedding as that token\n",
    "                                    last_true_embed = embeddings[curr_index]\n",
    "        \n",
    "                                avg_embed = np.mean(np.array([last_true_embed, embeddings[curr_index]]), axis=0)\n",
    "                                # set it to the list\n",
    "                                for x in range(i, curr_index):\n",
    "                                    final_embeddings[x] = avg_embed\n",
    "        \n",
    "                                # break out of the while loop since the average embedding have been added\n",
    "                                found_end_embed = True\n",
    "                                break\n",
    "                            else:\n",
    "                                curr_index += 1\n",
    "        \n",
    "                        if not found_end_embed:\n",
    "                            # the NaN value was not encapsulated by two known embeddings, so take the last known embedding and just use that one\n",
    "                            for x in range(i, len(embeddings)):\n",
    "                                final_embeddings[x] = last_true_embed\n",
    "\n",
    "        else:\n",
    "            # handle the vectors for which no information is known by using zero-vectors\n",
    "            final_embeddings = [np.zeros(embeddings_index.vector_size, dtype=np.float32) for x in embeddings]\n",
    "\n",
    "            # overwrite the embeddings for which we have an actual embedding\n",
    "            for i, embed in enumerate(embeddings):\n",
    "                if not np.isnan(embed).any():\n",
    "                    # we have a mean embedding so just store it and continue\n",
    "                    final_embeddings[i] = embeddings[i]\n",
    "\n",
    "    \n",
    "        # set the vectors to a numpy 2d array and save it\n",
    "        final_embedding = np.array(final_embeddings)\n",
    "\n",
    "        # save it\n",
    "        subject_output_path = os.path.join(features_output_path, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), final_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63e341c-4182-4f46-9b17-6d5226147de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and save the embeddings#\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "\n",
    "# extract the embeddings without extra keywords\n",
    "keyw_to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_word2vec:\n",
    "    # retrieve the embedding features with averaged unknown vectors\n",
    "    extract_text_segments(df_annotations, text_ref_dict, w2v_embeddings_index,\n",
    "                          keyw_to_remove, embeddings_save_folder, \"w2v_seconds_normal_avg\")\n",
    "\n",
    "    # retrieve the embedding features with unknown zero-vectors\n",
    "    extract_text_segments(df_annotations, text_ref_dict, w2v_embeddings_index,\n",
    "                          keyw_to_remove, embeddings_save_folder, \"w2v_seconds_normal_zero\", use_avg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f59e37db-a721-40aa-b3c1-4eace59298f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embedding with extra keywords\n",
    "# depression_keywords = [\"depression\", \"depressive\", \"antidepressant\", \"depressed\", \"anxiety\", \"psychiatrist\", \"ptsd\"]\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "keyw_to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "depression_keywords = [x for x in depri_synonyms if len(x.split()) == 1]\n",
    "depression_keywords.extend(keyw_to_remove)\n",
    "\n",
    "if run_word2vec:\n",
    "    extract_text_segments(df_annotations, text_ref_dict, w2v_embeddings_index,\n",
    "                          depression_keywords, embeddings_save_folder, \"w2v_seconds_keyw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42b7d84-08ae-48dc-8859-78b90d86f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averaged text embeddings\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "output_feature_name = \"w2v_seq_avg\"\n",
    "to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_word2vec:\n",
    "    for index, row in df_annotations.iterrows():\n",
    "    \n",
    "        video_id = row.video_id\n",
    "        final_embeddings = []\n",
    "    \n",
    "        texts = text_ref_dict.get(video_id).get(\"text_segments\")\n",
    "        for text in texts:\n",
    "            # clean up the words\n",
    "            cleaned_text = clean_text(text, replace_typical_misspell).split()\n",
    "    \n",
    "            # remove some of the keywords\n",
    "            cleaned_text = [word for word in cleaned_text if not word in to_remove]\n",
    "    \n",
    "            # disregard sentences with single words\n",
    "            if len(cleaned_text) <= 1:\n",
    "                continue\n",
    "            else:\n",
    "                # get the embedding\n",
    "                embedding = w2v_embeddings_index.get_mean_vector(cleaned_text)\n",
    "                final_embeddings.append(embedding)\n",
    "                \n",
    "        # save the embedding\n",
    "        final_embeddings = np.array(final_embeddings)\n",
    "\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc7d6ad-4463-4b52-9cdf-21860c611093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averaged text embeddings of sentences and remove the ones containing certain keywords\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "output_feature_name = \"w2v_seq_avg_keyw\"\n",
    "to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_word2vec:\n",
    "    for index, row in df_annotations.iterrows():\n",
    "    \n",
    "        video_id = row.video_id\n",
    "        final_embeddings = []\n",
    "    \n",
    "        texts = text_ref_dict.get(video_id).get(\"text_segments\")\n",
    "        for text in texts:\n",
    "            # clean up the words\n",
    "            cleaned_text = clean_text(text, replace_typical_misspell)\n",
    "\n",
    "            # check for each keyword\n",
    "            skip_sentence = False\n",
    "            for keyw in depri_synonyms:\n",
    "                if keyw in cleaned_text:\n",
    "                    skip_sentence = True\n",
    "                    break\n",
    "\n",
    "            if skip_sentence:\n",
    "                continue\n",
    "                \n",
    "            # remove some of the keywords\n",
    "            cleaned_text = [word for word in cleaned_text.split() if not word in to_remove]\n",
    "    \n",
    "            # disregard sentences with single words\n",
    "            if len(cleaned_text) <= 1:\n",
    "                continue\n",
    "            else:\n",
    "                # get the embedding\n",
    "                embedding = w2v_embeddings_index.get_mean_vector(cleaned_text)\n",
    "                final_embeddings.append(embedding)\n",
    "                \n",
    "        # save the embedding\n",
    "        final_embeddings = np.array(final_embeddings)\n",
    "\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffebe11-9daa-4557-ae91-ed63a3c0ee4f",
   "metadata": {},
   "source": [
    "## BioWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be86f2df-df42-4822-9e4b-8dca841041ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_biowordvec = True\n",
    "\n",
    "if run_biowordvec:\n",
    "    biowordvec_path = Path(r\"E:/master/embedding_models/bio_embedding_extrinsic\")\n",
    "    assert os.path.exists(biowordvec_path), \"embedding model not found\"\n",
    "\n",
    "    bio_embeddings_index = KeyedVectors.load_word2vec_format(biowordvec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e02e707-52fd-469f-9416-2f79f43050dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 20988/20988 [00:00<00:00, 327737.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 88.10% of vocab\n",
      "Found embeddings for  98.57% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, bio_embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a8594c-c4c1-4124-8bc6-c3744e9279c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve and save the embeddings\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "\n",
    "# extract the embeddings without extra keywords\n",
    "keyw_to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_biowordvec:\n",
    "    # retrieve the embedding features with averaged unknown vectors\n",
    "    extract_text_segments(df_annotations, text_ref_dict, bio_embeddings_index,\n",
    "                          keyw_to_remove, embeddings_save_folder, \"biow_seconds_normal_avg\")\n",
    "\n",
    "    # retrieve the embedding features with unknown zero-vectors\n",
    "    extract_text_segments(df_annotations, text_ref_dict, bio_embeddings_index,\n",
    "                          keyw_to_remove, embeddings_save_folder, \"biow_seconds_normal_zero\", use_avg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bd72702-1bd4-4bdd-a210-a0659845b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embedding with extra keywords\n",
    "# depression_keywords = [\"depression\", \"depressive\", \"antidepressant\", \"depressed\", \"anxiety\", \"psychiatrist\", \"ptsd\"]\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "keyw_to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "depression_keywords = [x for x in depri_synonyms if len(x.split()) == 1]\n",
    "depression_keywords.extend(keyw_to_remove)\n",
    "\n",
    "if run_biowordvec:\n",
    "    extract_text_segments(df_annotations, text_ref_dict, bio_embeddings_index,\n",
    "                          depression_keywords, embeddings_save_folder, \"biow_seconds_keyw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27dcd530-c8b8-4435-bb05-52d62cdec4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averaged text embeddings\n",
    "output_feature_name = \"biow_seq_avg\"\n",
    "to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_biowordvec:\n",
    "    for index, row in df_annotations.iterrows():\n",
    "    \n",
    "        video_id = row.video_id\n",
    "        final_embeddings = []\n",
    "    \n",
    "        texts = text_ref_dict.get(video_id).get(\"text_segments\")\n",
    "        for text in texts:\n",
    "            # clean up the words\n",
    "            cleaned_text = clean_text(text, replace_typical_misspell).split()\n",
    "    \n",
    "            # remove some of the keywords\n",
    "            cleaned_text = [word for word in cleaned_text if not word in to_remove]\n",
    "    \n",
    "            # disregard sentences with single words\n",
    "            if len(cleaned_text) <= 1:\n",
    "                continue\n",
    "            else:\n",
    "                # get the embedding\n",
    "                embedding = bio_embeddings_index.get_mean_vector(cleaned_text)\n",
    "                final_embeddings.append(embedding)\n",
    "                \n",
    "        # save the embedding\n",
    "        final_embeddings = np.array(final_embeddings)\n",
    "\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fb7e17d-ab3c-4be3-a6c1-b33c65f3145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the averaged text embeddings of sentences and remove the ones containing certain keywords\n",
    "embeddings_save_folder = Path(r\"E:/master/data/embeddings-dataset\")\n",
    "output_feature_name = \"biow_seq_avg_keyw\"\n",
    "to_remove = ['a', 'to', 'of', 'and']\n",
    "\n",
    "if run_biowordvec:\n",
    "    for index, row in df_annotations.iterrows():\n",
    "    \n",
    "        video_id = row.video_id\n",
    "        final_embeddings = []\n",
    "    \n",
    "        texts = text_ref_dict.get(video_id).get(\"text_segments\")\n",
    "        for text in texts:\n",
    "            # clean up the words\n",
    "            cleaned_text = clean_text(text, replace_typical_misspell)\n",
    "\n",
    "            # check for each keyword\n",
    "            skip_sentence = False\n",
    "            for keyw in depri_synonyms:\n",
    "                if keyw in cleaned_text:\n",
    "                    skip_sentence = True\n",
    "                    break\n",
    "\n",
    "            if skip_sentence:\n",
    "                continue\n",
    "                \n",
    "            # remove some of the keywords\n",
    "            cleaned_text = [word for word in cleaned_text.split() if not word in to_remove]\n",
    "    \n",
    "            # disregard sentences with single words\n",
    "            if len(cleaned_text) <= 1:\n",
    "                continue\n",
    "            else:\n",
    "                # get the embedding\n",
    "                embedding = bio_embeddings_index.get_mean_vector(cleaned_text)\n",
    "                final_embeddings.append(embedding)\n",
    "                \n",
    "        # save the embedding\n",
    "        final_embeddings = np.array(final_embeddings)\n",
    "\n",
    "        subject_output_path = os.path.join(embeddings_save_folder, str(video_id))\n",
    "        os.makedirs(subject_output_path, exist_ok=True)\n",
    "\n",
    "        np.save(os.path.join(subject_output_path, f\"{output_feature_name}.npy\"), final_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
